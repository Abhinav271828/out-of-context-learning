import torch
from torch.utils.data import Dataset
import random
from tqdm import tqdm


class MembershipFewShot(Dataset):
    """
    Create a dataset for the membership task in a few-shot setting.
    Parameters:
        num_examples (int): the number of examples per task.
        dataset_size (int): the number of tasks in the dataset.
        neg_samples (float): the fraction of negative samples per task. a number between 0 and 1.

    The dataset is generated by sampling a language L_i at random and generating num_examples examples from it.
    On average, neg_samples * num_examples negative examples are generated (randomly) and the rest are positive examples.
    Note that the probability of generating a positive sample by chance is negligible.
    Negative samples are marked as -1 and positive samples are marked as 1.
    This label is the last element of the input tensor.

    The test samples are generated with a 50% chance of being negative.
    Negative samples are generated by sampling a random length and filling it with random symbols from the alphabet, as
    well as a final 0, which is a placeholder for the classification label.
    Positive samples are sampled from the language L_i and similarly labelled as 0.

    The ground-truth labels are in a separate tensor.

    We also store the language L_i that was used to generate the task.
    """

    def __init__(self, num_examples, dataset_size, neg_samples=0.5):
        self.num_examples = num_examples
        self.dataset_size = dataset_size
        self.neg_samples = neg_samples

        self.data = torch.zeros(self.dataset_size, self.num_examples + 1, 21)
        self.labels = torch.zeros(self.dataset_size)
        self.languages = torch.zeros(self.dataset_size)
        self.alphabet = {"a": 0, "b": 1, "c": 2, "#": 3}
        self.pad_token = 3
        self.generate_data()

    def generate_data(self):
        tasks_per_language = self.dataset_size // 3 + 1
        for language in range(1, 4):
            with open(f"data/L{language+3}.txt", "r") as f:
                samples = f.readlines()
                for i in tqdm(
                    range(
                        (language - 1) * tasks_per_language,
                        min(language * tasks_per_language, self.dataset_size),
                    ),
                    desc=f"Generating data for language {language}",
                ):
                    self.languages[i] = language
                    # Inputs (examples)
                    for j in range(self.num_examples):
                        # Negative samples
                        if random.random() < self.neg_samples:
                            length = random.randint(1, 20)
                            self.data[i][j] = torch.tensor(
                                random.choices(sorted(self.alphabet.values()), k=length)
                                + [self.pad_token] * (20 - length)
                                + [-1]
                            )
                        # Positive samples
                        else:
                            sample = random.choice(samples).strip()
                            self.data[i][j] = torch.tensor(
                                [self.alphabet[c] for c in sample]
                                + [self.pad_token] * (20 - len(sample))
                                + [1]
                            )

                    # Test sample
                    # Negative
                    if random.random() < 0.5:
                        length = random.randint(1, 20)
                        self.data[i][self.num_examples] = torch.tensor(
                            random.choices(sorted(self.alphabet.values()), k=length)
                            + [self.pad_token] * (20 - length)
                            + [0]
                        )
                        self.labels[i] = -1
                    # Positive
                    else:
                        sample = random.choice(samples).strip()
                        self.data[i][self.num_examples] = torch.tensor(
                            [self.alphabet[c] for c in sample]
                            + [self.pad_token] * (20 - len(sample))
                            + [0]
                        )
                        self.labels[i] = 1
        print(self.labels.mean())

    def __len__(self):
        return self.dataset_size

    def __getitem__(self, idx):
        """
        Returns a pair ([num_examples+1, 21], [1])
        where
        - num_examples+1 is the number of examples in the task, including the test sample.
        - 21 is the length of the input vector, including the label.
        - 1 is the label of the test sample (whose label element is 0).
        """
        return self.data[idx], self.labels[idx]
